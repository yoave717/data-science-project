{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "from datetime import timedelta, date\n",
    "import re\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import folium\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import KDTree\n",
    "from numba import jit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# The stride client library, used to make the calls to the stride api\n",
    "# import stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data location based on current workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = './2023_siri_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './drive/MyDrive/2023_siri_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'G:\\\\My Drive\\\\2023_siri_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Our data is sourced from the Open-Bus Stride ETL packages. These packages provide data in hourly intervals. The data includes information about bus locations, the nearest stop to each bus location, and correlations to the GTFS ride. This includes the date and time of scheduled arrival times, among other details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download Function\n",
    "\n",
    "In this cell, we define a function `download_files` to download and extract the data files for a given year.\n",
    "\n",
    "The function iterates over each day and hour in the year, constructs the corresponding file URL, and sends a GET request to download the file. If the file is successfully downloaded, it is saved in the 'compressed' directory and then extracted to the 'data' directory.\n",
    "\n",
    "If the file is not found on the server, its URL is added to a list of missing files, which is printed at the end of the function.\n",
    "\n",
    "The function also reads and writes the last downloaded date and hour from/to a file, allowing the download process to be resumed if it is interrupted.\n",
    "\n",
    "Finally, we call the function to download the files for the year 2023.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "def download_files(year):\n",
    "    start_date = date(year, 1, 1)\n",
    "    end_date = date(year+1, 1, 1)\n",
    "    missing_files = []\n",
    "\n",
    "    if not os.path.exists(f'{DATA_FOLDER}/compressed'):\n",
    "        os.makedirs(f'{DATA_FOLDER}/compressed')\n",
    "    if not os.path.exists(f'{DATA_FOLDER}/data'):\n",
    "        os.makedirs(f'{DATA_FOLDER}/data')\n",
    "\n",
    "    # Read the start date and hour from a file\n",
    "    try:\n",
    "        with open(f'{DATA_FOLDER}/last_downloaded.txt', 'r') as f:\n",
    "            last_downloaded = f.read().strip()\n",
    "            start_date = date(int(last_downloaded[:4]), int(last_downloaded[5:7]), int(last_downloaded[8:10]))\n",
    "            start_hour = int(last_downloaded[11:13])\n",
    "    except FileNotFoundError:\n",
    "        start_hour = 0\n",
    "\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        for hour in range(start_hour, 24):\n",
    "            filename = f\"{single_date.strftime('%Y-%m-%d')}.{str(hour).zfill(2)}\"\n",
    "            url = f\"https://s3.eu-west-2.wasabisys.com/stride/stride-etl-packages/siri/{single_date.strftime('%Y/%m')}/{filename}.zip\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(f\"{DATA_FOLDER}/compressed/{filename}.zip\", 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                with zipfile.ZipFile(f\"{DATA_FOLDER}/compressed/{filename}.zip\", 'r') as zip_ref:\n",
    "                    if f\"{filename}.csv\" in zip_ref.namelist():\n",
    "                        zip_ref.extract(f\"{filename}.csv\", path=f'{DATA_FOLDER}/data')\n",
    "                # Save the current date and hour to a file\n",
    "                with open(f'{DATA_FOLDER}/last_downloaded.txt', 'w') as f:\n",
    "                    f.write(f\"{single_date.strftime('%Y-%m-%d')}.{str(hour).zfill(2)}\")\n",
    "            else:\n",
    "                missing_files.append(url)\n",
    "        start_hour = 0\n",
    "\n",
    "    print(\"Missing files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_files(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the data intact\n",
    "\n",
    "We might have a missing file in the data folder\n",
    "This function checks for missing files based on the compressed files folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def extract_missing_csv(data_folder):\n",
    "    zip_dir = f'{data_folder}/compressed'\n",
    "    csv_dir = f'{data_folder}/data'\n",
    "    # Get a list of all ZIP files\n",
    "    zip_files = glob.glob(f'{zip_dir}/*.zip')\n",
    "    zip_files.sort()\n",
    "\n",
    "    for zip_file in zip_files:\n",
    "        # Get the corresponding CSV file name\n",
    "        csv_file_name = os.path.basename(zip_file)[:-4] + '.csv'\n",
    "        csv_file_path = f'{csv_dir}/{csv_file_name}'\n",
    "        \n",
    "        # If the CSV file does not exist\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                # Extract the CSV file\n",
    "                if csv_file_name in zip_ref.namelist():\n",
    "                    zip_ref.extract(csv_file_name, path=csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_missing_csv(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Segmentation\n",
    "\n",
    "This cell is designed to efficiently manage and process large datasets by breaking them down into more manageable chunks, optimizing memory usage, and enhancing data accessibility. The process involves several key steps:\n",
    "\n",
    "1. **Data Segmentation**: The script segments large CSV files into smaller Parquet files, each containing up to 10 million rows. This segmentation facilitates easier handling of large datasets, as Parquet files are more efficient in terms of storage and speed when accessing subsets of data.\n",
    "\n",
    "2. **Type Conversion and Cleanup**: It converts data types for specific columns to ensure consistency and optimizes memory usage. For example, it converts certain columns to string, integer, and floating-point types as needed. Additionally, it processes date columns to ensure they are in the correct datetime format.\n",
    "\n",
    "3. **Column Removal**: To streamline the dataset, the script removes columns that are not immediately necessary for analysis. This step reduces file size and simplifies the dataset, making it easier to work with. The columns to be removed are specified in the [`columns_to_drop`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22%2FUsers%2Fyoavelkayam%2FDocuments%2FOpenU%2F%D7%A1%D7%93%D7%A0%D7%94%20%D7%91%D7%9E%D7%93%D7%A2%D7%99%20%D7%94%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D%2Fdata-science-project%2Fproject.ipynb%22%2C%22path%22%3A%22%2FUsers%2Fyoavelkayam%2FDocuments%2FOpenU%2F%D7%A1%D7%93%D7%A0%D7%94%20%D7%91%D7%9E%D7%93%D7%A2%D7%99%20%D7%94%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D%2Fdata-science-project%2Fproject.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X20sZmlsZQ%3D%3D%22%7D%2C%7B%22line%22%3A67%2C%22character%22%3A4%7D%5D \"/Users/yoavelkayam/Documents/OpenU/סדנה במדעי הנתונים/data-science-project/project.ipynb\") list.\n",
    "\n",
    "4. **File Management**: The script includes functionality to manage files efficiently. It removes the original CSV files after processing them into Parquet format to free up storage space. It also handles the continuation of data processing from where it left off, by identifying the last processed file and its position, thus avoiding reprocessing of data.\n",
    "\n",
    "5. **Data Deduplication**: Before appending new data to the dataset, it removes duplicate entries to ensure the uniqueness of records.\n",
    "\n",
    "6. **Dynamic File Naming**: The output Parquet files are named dynamically based on their content, indicating the range of data they contain. This naming convention makes it easier to identify and access specific segments of data.\n",
    "\n",
    "7. **Efficient Data Concatenation**: The script concatenates new data to an existing DataFrame until the specified row limit is reached. Once the limit is reached, the data is saved to a Parquet file, and the process continues with the remaining data.\n",
    "\n",
    "This approach not only optimizes the processing of large datasets by reducing memory overhead but also organizes the data into a more accessible and efficient format, facilitating faster data retrieval and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_folder_size(folder_path):\n",
    "    total = 0\n",
    "    for path, dirs, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            fp = os.path.join(path, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'output_(\\d+)_from_(.+)_at_(\\d+)_to_(.+)_at_(\\d+)', file_name)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def extract_variables(file_name):\n",
    "    match = re.search(r'output_(\\d+)_from_(.+)_at_(\\d+)_to_(.+)_at_(\\d+)', file_name)\n",
    "    if match:\n",
    "        x = int(match.group(1))\n",
    "        start_file = match.group(2)\n",
    "        start_pos = int(match.group(3))\n",
    "        end_file = match.group(4)\n",
    "        end_pos = int(match.group(5))\n",
    "        return x, start_file, start_pos, end_file, end_pos\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_df_to_parquet(df, file_counter, start_file, start_pos, last_file, end_pos, files_to_remove, location):\n",
    "    print(f'Saving file {file_counter}...')\n",
    "    \n",
    "    dtypes = {\n",
    "        'id': 'string',\n",
    "        'bearing': 'int32',\n",
    "        'lat': 'float64',\n",
    "        'lon': 'float64',\n",
    "        'gtfs_stop_lat': 'float64',\n",
    "        'gtfs_stop_lon': 'float64',\n",
    "        }\n",
    "    \n",
    "    date_cols = ['recorded_at_time', 'siri_scheduled_start_time', 'gtfs_start_time', 'gtfs_end_time', 'gtfs_arrival_time', 'gtfs_departure_time']\n",
    "    \n",
    "    # converting types\n",
    "    df = df.astype(dtypes)\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "    \n",
    "    start_file_name = os.path.splitext(os.path.basename(start_file))[0]\n",
    "    last_file_name = os.path.splitext(os.path.basename(last_file))[0]\n",
    "\n",
    "    file_name = f'{location}/output_{file_counter}_from_{start_file_name}_at_{start_pos}_to_{last_file_name}_at_{end_pos}.parquet'\n",
    "\n",
    "    df.to_parquet(file_name, index=False)\n",
    "\n",
    "    # remove csv files\n",
    "    print(f'Processed files {os.path.basename(files_to_remove[0])} to {os.path.basename(files_to_remove[-2])}. Now Deleting...')\n",
    "    while len(files_to_remove) > 1:\n",
    "        os.remove(files_to_remove[0])\n",
    "        files_to_remove.pop(0)\n",
    "\n",
    "    if end_pos == -1 and files_to_remove:\n",
    "        os.remove(files_to_remove[0])\n",
    "        files_to_remove.pop(0)\n",
    "\n",
    "def process_files(folder_path, rows_per_file=10000000):\n",
    "    print(f\"Folder size before processing: {get_folder_size(folder_path)} bytes\")\n",
    "\n",
    "    output_files_folder_path = f'{folder_path}\\\\concatenated_data_parquet\\\\'\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    columns_to_drop = ['gtfs_agency_name', 'gtfs_stop_name', 'gtfs_route_long_name', 'gtfs_line_ref', 'gtfs_operator_ref', 'distance_from_siri_ride_stop_meters', 'distance_from_journey_start']\n",
    "\n",
    "    start_file = None\n",
    "    start_pos = None\n",
    "    last_file = None\n",
    "    end_pos = None\n",
    "\n",
    "    csv_files = glob.glob(f'{folder_path}\\\\data\\\\*.csv')\n",
    "\n",
    "    csv_files.sort()\n",
    "\n",
    "    file_counter = 1\n",
    "\n",
    "    output_files = glob.glob(f'{output_files_folder_path}output_*.parquet')\n",
    "\n",
    "    if output_files:\n",
    "        last_output_file = max(output_files, key=extract_number)\n",
    "        file_counter, start_file, start_pos, last_file, end_pos = extract_variables(last_output_file)\n",
    "        last_df = pd.read_parquet(last_output_file)\n",
    "        # If the last output file contains less than max rows, load it into df\n",
    "        if len(last_df) < rows_per_file:\n",
    "            df = last_df\n",
    "            os.remove(last_output_file)  # remove the last file as it will be rewritten later\n",
    "        else:\n",
    "            start_file = last_file\n",
    "            start_pos = end_pos\n",
    "            file_counter += 1\n",
    "    \n",
    "    if last_file is not None:\n",
    "        start_index = csv_files.index(f'{folder_path}\\\\data\\\\{last_file}.csv')\n",
    "    else:\n",
    "        start_index = 0\n",
    "        \n",
    "    files_to_remove = []\n",
    "    for file in csv_files[start_index:]:\n",
    "        files_to_remove.append(file)\n",
    "        # If the file is not empty\n",
    "        if os.path.getsize(file) > 0:\n",
    "            print(f'On file {file}')\n",
    "            try:\n",
    "                if end_pos is None:\n",
    "                    end_pos = 0\n",
    "                if start_file is None:\n",
    "                    start_file = file\n",
    "\n",
    "                \n",
    "                if csv_files.index(file) == start_index:\n",
    "                    temp_df = pd.read_csv(file, dtype='string', skiprows=range(1, end_pos))\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(file, dtype='string')\n",
    "                print(temp_df.shape[0])\n",
    "                temp_df['original_file'] = os.path.basename(file)  # Add the original file name to each row\n",
    "                last_file = file\n",
    "                \n",
    "                # Remove duplicates\n",
    "                temp_df = temp_df.drop_duplicates()\n",
    "\n",
    "                # Drop the unnecessary columns\n",
    "                temp_df = temp_df.drop(columns=columns_to_drop)\n",
    "                df = pd.concat([df, temp_df])\n",
    "                \n",
    "                # If the main DataFrame has reached max rows\n",
    "                print(df.shape[0], temp_df.shape[0])\n",
    "                if df.shape[0] >= rows_per_file:\n",
    "                    start_pos = end_pos\n",
    "                    end_pos = df.shape[0] - rows_per_file\n",
    "                    save_df_to_parquet(df[:rows_per_file], file_counter, start_file, start_pos, last_file, end_pos, files_to_remove, output_files_folder_path)\n",
    "\n",
    "                    # Keep the remaining rows in the DataFrame\n",
    "                    df = df[rows_per_file:]\n",
    "                    file_counter += 1\n",
    "                    start_file = file\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"File {file} is empty or only contains a header.\")\n",
    "        \n",
    "    # Write the remaining rows in the DataFrame to a parquet file\n",
    "    if not df.empty:\n",
    "        save_df_to_parquet(df, file_counter, start_file, start_pos, last_file, -1, files_to_remove, output_files_folder_path)\n",
    "\n",
    "    print(f\"Folder size after processing: {get_folder_size(folder_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "process_files(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Examination\n",
    "\n",
    "In this phase, we will focus on analyzing a single bus line, specifically line 68, which operates from Kiryat Ono terminal to Tel-Aviv central station. Our analysis will be limited to the first month of 2023.\n",
    "\n",
    "Our primary goal is to identify instances of delays and subsequently investigate potential causes for these delays.\n",
    "\n",
    "To facilitate this, we will prepare a DataFrame that consolidates all relevant data pertaining to this bus line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def create_filtered_df(parquet_dir, line_refs, limit=0, offset=0):\n",
    "    parquet_files = glob.glob(f'{parquet_dir}/output_*.parquet')\n",
    "\n",
    "    parquet_files.sort(key=extract_number)\n",
    "    \n",
    "    if limit > 0:\n",
    "        parquet_files = parquet_files[offset:offset+limit]\n",
    "    else:\n",
    "        parquet_files = parquet_files[offset:]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for file in parquet_files:\n",
    "        # Read the Parquet file\n",
    "        temp_df = pd.read_parquet(file, filters=[('siri_line_ref', 'in', line_refs)])\n",
    "        temp = temp_df.astype({'gtfs_stop_lat': float, 'gtfs_stop_lon': float, 'lat': float, 'lon': float})\n",
    "        df = pd.concat([df, temp_df])\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 20 == 0:\n",
    "            print(f'Processed {counter} files. Current file: {os.path.basename(file)}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_filtered_df(f'{DATA_FOLDER}/concatenated_data_parquet', ['987'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12396"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Formatting Data\n",
    "\n",
    "In this step, we sort the dataframe `df` by 'siri_journey_ref' and 'recorded_at_time'. We also convert the 'gtfs_arrival_time' and 'recorded_at_time' columns to datetime format for easier manipulation in later steps.\n",
    "\n",
    "We then create two new dataframes:\n",
    "\n",
    "- `df_bus_journey_stops`: This dataframe is created by selecting specific columns from `df_sorted` and dropping duplicates. It contains information about the bus journey stops.\n",
    "\n",
    "- `df_locations`: This dataframe is created by excluding columns that start with 'gtfs' from `df_sorted`. It contains information about the bus locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    df_sorted = df.sort_values(['siri_journey_ref', 'recorded_at_time'])\n",
    "    \n",
    "    bus_journey_cols = ['id','siri_journey_ref', 'siri_vehicle_ref', 'siri_stop_code', 'siri_stop_order', 'siri_operator_ref', 'siri_line_ref', 'gtfs_journey_ref', 'gtfs_start_time', 'gtfs_end_time', 'gtfs_stop_code', 'gtfs_stop_lat', 'gtfs_stop_lon', 'gtfs_stop_city', 'gtfs_arrival_time', 'gtfs_stop_sequence', 'gtfs_route_short_name', 'gtfs_route_direction', 'gtfs_route_mkt']\n",
    "    df_bus_journey_stops = df_sorted[bus_journey_cols].drop_duplicates()\n",
    "\n",
    "    # Remove rows where 'gtfs_stop_lat' or 'gtfs_stop_lon' is NaN\n",
    "    df_bus_journey_stops = df_bus_journey_stops.dropna(subset=['gtfs_stop_lat', 'gtfs_stop_lon'])\n",
    "\n",
    "    location_cols = [col for col in df_sorted.columns if not col.startswith('gtfs')]\n",
    "    df_locations = df_sorted[location_cols]\n",
    "\n",
    "     # Remove journeys where all locations have the same lat and lon\n",
    "    journey_counts = df_locations.groupby('siri_journey_ref')[['lat', 'lon']].nunique()\n",
    "    journeys_to_keep = journey_counts[(journey_counts['lat'] > 2) | (journey_counts['lon'] > 2)].index\n",
    "    df_locations = df_locations[df_locations['siri_journey_ref'].isin(journeys_to_keep)]\n",
    "    df_bus_journey_stops = df_bus_journey_stops[df_bus_journey_stops['siri_journey_ref'].isin(journeys_to_keep)]\n",
    "\n",
    "\n",
    "    return df_bus_journey_stops, df_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bus_journey_stops, df_locations = process_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Distance and Speed\n",
    "\n",
    "In this step, we define a function `calculate_distance_and_speed` to calculate the distance and speed for each journey.\n",
    "\n",
    "- The function first calculates the distances between consecutive locations using the `geodesic` function from the `geopy.distance` module.\n",
    "- It then calculates the time difference between consecutive locations and uses this to calculate the speed.\n",
    "- Any `NaN` values in the 'speed', 'distance', and 'time_diff' columns are replaced with 0.\n",
    "\n",
    "Finally, we apply this function to our DataFrame `df_locations` using the `groupby` and `apply` methods, and reset the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-calculate constants outside the function\n",
    "earth_radius = 6367 * 2 * np.pi\n",
    "\n",
    "@jit(nopython=True)\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    # Convert coordinates to radians (vectorized)\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula (vectorized operations)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return earth_radius * c\n",
    "\n",
    "def calculate_distance_and_speed(group):\n",
    "    \n",
    "    lats = group['lat'].values\n",
    "    lons = group['lon'].values\n",
    "\n",
    "    # Vectorized haversine calculation\n",
    "    distances = haversine(lats[:-1], lons[:-1], lats[1:], lons[1:])\n",
    "\n",
    "    # Add distance and time difference columns\n",
    "    group['distance'] = np.append(distances, np.nan)\n",
    "    group['time_diff'] = (group['recorded_at_time'].shift(-1) - group['recorded_at_time']).dt.total_seconds() / 3600\n",
    "\n",
    "    # Calculate speed\n",
    "    group['speed'] = group['distance'] / np.maximum(group['time_diff'], 1e-9)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    group['speed'] = np.nan_to_num(group['speed'])\n",
    "    group['distance'] = np.nan_to_num(group['distance'])\n",
    "    group['time_diff'] = np.nan_to_num(group['time_diff'])\n",
    "\n",
    "    return group\n",
    "\n",
    "def process_locations(df_locations):\n",
    "    df_locations = df_locations.groupby('siri_journey_ref').apply(calculate_distance_and_speed)\n",
    "    df_locations.reset_index(drop=True, inplace=True)\n",
    "    return df_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = process_locations(df_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Arrival Times\n",
    "\n",
    "In this step, we define several functions to calculate the estimated arrival times at each bus stop for each journey.\n",
    "\n",
    "- The `haversine` function calculates the distance between two points on the Earth's surface given their latitudes and longitudes.\n",
    "- The `calculate_bearing` function calculates the bearing between two points on the Earth's surface.\n",
    "- The `calculate_arrival_times` function uses the above two functions to estimate the arrival times at each bus stop for each journey. It takes into account the speed of the bus, the distance to the next stop, and the direction of the bus.\n",
    "\n",
    "An important part of the `calculate_arrival_times` function is the calculation of the 'moving_towards_stop' field. This field is a boolean that indicates whether the bus is moving towards or away from the stop. It is calculated by comparing the bearing of the bus to the bearing of the line from the bus to the stop. If the bus's bearing is within a certain range of the bearing to the stop, then the bus is considered to be moving towards the stop. This calculation is important because it allows us to distinguish between a bus that is approaching a stop and a bus that has already passed a stop.\n",
    "\n",
    "The 'distance_from_stop' field is the distance from the closest bus location to the stop. It is calculated using the `haversine` function.\n",
    "\n",
    "The 'estimated_arrival_time' field is the estimated time that the bus will arrive at the stop. It is calculated by adding the estimated time to the stop (which is the distance to the stop divided by the speed of the bus) to the time that the closest location was recorded. If the bus is moving away from the stop, the estimated time to the stop is subtracted from the recorded time instead.\n",
    "\n",
    "Finally, we call the `calculate_arrival_times` function to calculate the estimated arrival times for our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Calculate the difference in longitudes\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Calculate the bearing\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "    bearing = np.degrees(np.arctan2(x, y))\n",
    "\n",
    "    # Normalize the bearing to be between 0 and 360\n",
    "    return (bearing + 360) % 360\n",
    "\n",
    "def calculate_arrival_times(df_locations, df_bus_journey_stops, bearing_threshold=15):\n",
    "    results = []\n",
    "\n",
    "    df_locations['speed'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    grouped_locations = df_locations.groupby('siri_journey_ref')\n",
    "\n",
    "    for journey, journey_locations in grouped_locations:\n",
    "        journey_stops = df_bus_journey_stops[df_bus_journey_stops['siri_journey_ref'] == journey]\n",
    "\n",
    "        # Skip the journey if it only has 2 or fewer stops\n",
    "        if len(journey_stops) <= 2:\n",
    "            continue\n",
    "        \n",
    "        journey_locations = journey_locations.sort_values('recorded_at_time')\n",
    "\n",
    "        # Interpolate to fill NaN values with the nearest non-NaN values\n",
    "        journey_locations['speed'].interpolate(method='nearest', limit_direction='both', inplace=True)\n",
    "\n",
    "        # Calculate the mean speed excluding the start and end points\n",
    "        mean_speed = journey_locations['speed'][1:-1].mean()\n",
    "\n",
    "        # Fill remaining NaN values at the start and end of the series with the mean speed\n",
    "        journey_locations['speed'].fillna(mean_speed, inplace=True)\n",
    "\n",
    "        latitudes = journey_locations['lat'].values\n",
    "        longitudes = journey_locations['lon'].values\n",
    "\n",
    "        journey_tree = KDTree(list(zip(latitudes, longitudes)), leafsize=4)\n",
    "\n",
    "        for i, stop in journey_stops.iterrows():\n",
    "            _, closest_location_idx = journey_tree.query((stop['gtfs_stop_lat'], stop['gtfs_stop_lon']))\n",
    "            closest_location = journey_locations.iloc[closest_location_idx]\n",
    "\n",
    "            bearing = calculate_bearing(closest_location['lat'], closest_location['lon'],\n",
    "                                         stop['gtfs_stop_lat'], stop['gtfs_stop_lon'])\n",
    "\n",
    "            distance_to_stop = haversine(closest_location['lat'], closest_location['lon'],\n",
    "                                          stop['gtfs_stop_lat'], stop['gtfs_stop_lon'])\n",
    "\n",
    "            moving_towards_stop = abs(closest_location['bearing'] - bearing) > bearing_threshold\n",
    "\n",
    "            estimated_time_to_stop = (distance_to_stop / closest_location['speed']) * 3600\n",
    "\n",
    "            if not moving_towards_stop:\n",
    "                estimated_arrival_time = closest_location['recorded_at_time'] - pd.Timedelta(seconds=estimated_time_to_stop)\n",
    "            else:\n",
    "                estimated_arrival_time = closest_location['recorded_at_time'] + pd.Timedelta(seconds=estimated_time_to_stop)\n",
    "\n",
    "            results.append({\n",
    "                'siri_journey_ref': journey,\n",
    "                'gtfs_stop_code': stop['gtfs_stop_code'],\n",
    "                'closest_location': closest_location['id'],\n",
    "                'closest_location_lat': closest_location['lat'],\n",
    "                'closest_location_lon': closest_location['lon'],\n",
    "                'closet_location_speed': closest_location['speed'],\n",
    "                'closest_location_bearing': closest_location['bearing'],\n",
    "                'recorded_at_time': closest_location['recorded_at_time'],\n",
    "                'distance_from_stop': distance_to_stop,\n",
    "                'scheduled_arrival_time': stop['gtfs_arrival_time'],\n",
    "                'estimated_arrival_time': estimated_arrival_time,\n",
    "                'moving_towards_stop': moving_towards_stop,\n",
    "                'arrival_time_diff': estimated_arrival_time - stop['gtfs_arrival_time']\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = pd.merge(df_bus_journey_stops, df_results, on=['siri_journey_ref', 'gtfs_stop_code'])\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_arrival_times(df_locations, df_bus_journey_stops, bearing_threshold=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bus Locations and Stops for a Specific Journey\n",
    "\n",
    "In this cell, we are creating a map to visualize the bus locations and stops for a specific journey. This map is centered at the mean latitude and longitude of the journey's locations.\n",
    "\n",
    "Each bus location is marked with a blue icon, and each stop location is marked with a red icon. If the bus is moving towards the stop, the corresponding marker's popup will show 'Moving Towards Stop: Yes', otherwise it will show 'Moving Towards Stop: No'.\n",
    "\n",
    "We also draw a blue line connecting all the bus locations and a red line connecting all the stop locations. This visual representation helps us to better understand the 'moving_towards_stop' field that was calculated earlier.\n",
    "\n",
    "By visualizing a specific journey, we can manually verify if the 'moving_towards_stop' calculation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_journey(siri_journey_ref, results):\n",
    "    # Filter the results for the specified journey\n",
    "    journey_results = results[results['siri_journey_ref'] == siri_journey_ref]\n",
    "\n",
    "    # Create a map centered at the mean latitude and longitude of the journey's locations\n",
    "    m = folium.Map(location=[journey_results['closest_location_lat'].mean(), journey_results['closest_location_lon'].mean()], zoom_start=13)\n",
    "\n",
    "    # Add a marker for each bus location\n",
    "    for _, row in journey_results.iterrows():\n",
    "        # Determine whether the bus has already left the stop\n",
    "        moving_towards_stop = 'Yes' if row['moving_towards_stop'] else 'No'\n",
    "        \n",
    "        # Add a marker with a popup showing whether the bus has left the stop\n",
    "        folium.Marker([row['closest_location_lat'], row['closest_location_lon']], \n",
    "                      icon=folium.Icon(color=\"blue\"), \n",
    "                      popup=f'Moving Towards Stop: {moving_towards_stop}').add_to(m)\n",
    "        \n",
    "    # Add a marker for each stop location\n",
    "    for _, row in journey_results.iterrows():\n",
    "        folium.Marker([row['gtfs_stop_lat'], row['gtfs_stop_lon']], icon=folium.Icon(color=\"red\")).add_to(m)\n",
    "\n",
    "    # Add a line for the bus locations\n",
    "    bus_locations = journey_results[['closest_location_lat', 'closest_location_lon']].values.tolist()\n",
    "    folium.PolyLine(bus_locations, color=\"blue\").add_to(m)\n",
    "\n",
    "    # Add a line for the stop locations\n",
    "    stop_locations = journey_results[['gtfs_stop_lat', 'gtfs_stop_lon']].values.tolist()\n",
    "    folium.PolyLine(stop_locations, color=\"red\").add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_9d2f5dc51a2a2f2e025514c9394e72f9 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_9d2f5dc51a2a2f2e025514c9394e72f9&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_9d2f5dc51a2a2f2e025514c9394e72f9 = L.map(\n",
       "                &quot;map_9d2f5dc51a2a2f2e025514c9394e72f9&quot;,\n",
       "                {\n",
       "                    center: [32.06092073333334, 34.855879333333334],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 13,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_0c095db3c177f2634ac84352880cd70b = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_0c095db3c177f2634ac84352880cd70b.addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var marker_65ab7985ff4151aeadda7afa403f0cc3 = L.marker(\n",
       "                [32.0825, 34.845043],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_2c1ee01cec6b1a6a18dd8405bac2fef9 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_65ab7985ff4151aeadda7afa403f0cc3.setIcon(icon_2c1ee01cec6b1a6a18dd8405bac2fef9);\n",
       "        \n",
       "    \n",
       "        var popup_bd5703c34bf94b012b2fa5a68df09925 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_89212e225f20a07b8133caab7bb44032 = $(`&lt;div id=&quot;html_89212e225f20a07b8133caab7bb44032&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_bd5703c34bf94b012b2fa5a68df09925.setContent(html_89212e225f20a07b8133caab7bb44032);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_65ab7985ff4151aeadda7afa403f0cc3.bindPopup(popup_bd5703c34bf94b012b2fa5a68df09925)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_782a33268c37b44df8181a777a0bb7a1 = L.marker(\n",
       "                [32.07529, 34.843635],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_c72e266375331c5b558ddb074b2536da = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_782a33268c37b44df8181a777a0bb7a1.setIcon(icon_c72e266375331c5b558ddb074b2536da);\n",
       "        \n",
       "    \n",
       "        var popup_7a9086c9bb6553ccee37e994a1ddbfd4 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_e89dadb2456e179bced2e2d8acf090df = $(`&lt;div id=&quot;html_e89dadb2456e179bced2e2d8acf090df&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_7a9086c9bb6553ccee37e994a1ddbfd4.setContent(html_e89dadb2456e179bced2e2d8acf090df);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_782a33268c37b44df8181a777a0bb7a1.bindPopup(popup_7a9086c9bb6553ccee37e994a1ddbfd4)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_62187fa08a349f77e517fb4cdb6cc556 = L.marker(\n",
       "                [32.068443, 34.840351],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_3c52e09c1a6bee3230246ba8d380b978 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_62187fa08a349f77e517fb4cdb6cc556.setIcon(icon_3c52e09c1a6bee3230246ba8d380b978);\n",
       "        \n",
       "    \n",
       "        var popup_6ad5c9a5a1ba73d55486fc44dc3617b0 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_060d90bd883e3b4c71cb512dc9812a99 = $(`&lt;div id=&quot;html_060d90bd883e3b4c71cb512dc9812a99&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_6ad5c9a5a1ba73d55486fc44dc3617b0.setContent(html_060d90bd883e3b4c71cb512dc9812a99);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_62187fa08a349f77e517fb4cdb6cc556.bindPopup(popup_6ad5c9a5a1ba73d55486fc44dc3617b0)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_238b317697c223482a84cd941c84e449 = L.marker(\n",
       "                [32.064876, 34.84473],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_0beffd164677435051fb981e113f762a = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_238b317697c223482a84cd941c84e449.setIcon(icon_0beffd164677435051fb981e113f762a);\n",
       "        \n",
       "    \n",
       "        var popup_3d5f4a86958810e320ee120f430b9aa9 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_57466c164d8954c8e0594523cbc9f8ac = $(`&lt;div id=&quot;html_57466c164d8954c8e0594523cbc9f8ac&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_3d5f4a86958810e320ee120f430b9aa9.setContent(html_57466c164d8954c8e0594523cbc9f8ac);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_238b317697c223482a84cd941c84e449.bindPopup(popup_3d5f4a86958810e320ee120f430b9aa9)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_3e82072f3ca1798c36bce81f4a927985 = L.marker(\n",
       "                [32.066123, 34.849441],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_42bf5b2fb8955ac5c4fd8044993a7097 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_3e82072f3ca1798c36bce81f4a927985.setIcon(icon_42bf5b2fb8955ac5c4fd8044993a7097);\n",
       "        \n",
       "    \n",
       "        var popup_a04cef8caa48544430eb2662f93db914 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_9318a19a0abbc004e432c38edabff3b9 = $(`&lt;div id=&quot;html_9318a19a0abbc004e432c38edabff3b9&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_a04cef8caa48544430eb2662f93db914.setContent(html_9318a19a0abbc004e432c38edabff3b9);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_3e82072f3ca1798c36bce81f4a927985.bindPopup(popup_a04cef8caa48544430eb2662f93db914)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_009ea569cf86e90afafc58476948a646 = L.marker(\n",
       "                [32.065547, 34.854652],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_f64f14b566f7d7f2212b6f821cdf23e7 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_009ea569cf86e90afafc58476948a646.setIcon(icon_f64f14b566f7d7f2212b6f821cdf23e7);\n",
       "        \n",
       "    \n",
       "        var popup_1ce6bfd5c0bcd82d423d1f065d38fde7 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_e608f26882bfcca967ebe9056769584b = $(`&lt;div id=&quot;html_e608f26882bfcca967ebe9056769584b&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_1ce6bfd5c0bcd82d423d1f065d38fde7.setContent(html_e608f26882bfcca967ebe9056769584b);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_009ea569cf86e90afafc58476948a646.bindPopup(popup_1ce6bfd5c0bcd82d423d1f065d38fde7)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_21dc5e9bc70c93ed97c01272c2d030e0 = L.marker(\n",
       "                [32.063648, 34.857154],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_e8ed89e9062655f0b6659ca1bc355c94 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_21dc5e9bc70c93ed97c01272c2d030e0.setIcon(icon_e8ed89e9062655f0b6659ca1bc355c94);\n",
       "        \n",
       "    \n",
       "        var popup_30e7f76075c01cbd0947376d97a65640 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_79a68612c6d995a634b6b1e8bb9a4c3d = $(`&lt;div id=&quot;html_79a68612c6d995a634b6b1e8bb9a4c3d&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_30e7f76075c01cbd0947376d97a65640.setContent(html_79a68612c6d995a634b6b1e8bb9a4c3d);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_21dc5e9bc70c93ed97c01272c2d030e0.bindPopup(popup_30e7f76075c01cbd0947376d97a65640)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_4daa20a205662f0fda2954514582cb43 = L.marker(\n",
       "                [32.063495, 34.861362],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_9d6b9b758aed7ed1360401386874b794 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_4daa20a205662f0fda2954514582cb43.setIcon(icon_9d6b9b758aed7ed1360401386874b794);\n",
       "        \n",
       "    \n",
       "        var popup_aeefbb49d215d0de702e7546bb357355 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_18b4b986f3c0ad1219114d082661081c = $(`&lt;div id=&quot;html_18b4b986f3c0ad1219114d082661081c&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_aeefbb49d215d0de702e7546bb357355.setContent(html_18b4b986f3c0ad1219114d082661081c);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_4daa20a205662f0fda2954514582cb43.bindPopup(popup_aeefbb49d215d0de702e7546bb357355)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_7862a690d9ef25083a99714f05df3656 = L.marker(\n",
       "                [32.060867, 34.86301],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_61690badc88d45d159fd7e7cbc5ede72 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_7862a690d9ef25083a99714f05df3656.setIcon(icon_61690badc88d45d159fd7e7cbc5ede72);\n",
       "        \n",
       "    \n",
       "        var popup_12d67a8f3b6e58cf2e92aff1e2d27e6c = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_bd158d83121188546962da38d5c1ba46 = $(`&lt;div id=&quot;html_bd158d83121188546962da38d5c1ba46&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_12d67a8f3b6e58cf2e92aff1e2d27e6c.setContent(html_bd158d83121188546962da38d5c1ba46);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_7862a690d9ef25083a99714f05df3656.bindPopup(popup_12d67a8f3b6e58cf2e92aff1e2d27e6c)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_a34573d0858c2d2a6236fa2a32846915 = L.marker(\n",
       "                [32.057689, 34.864395],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_83c5b567e9a0ed89fd7aca94c5ed9d21 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_a34573d0858c2d2a6236fa2a32846915.setIcon(icon_83c5b567e9a0ed89fd7aca94c5ed9d21);\n",
       "        \n",
       "    \n",
       "        var popup_f426e6eb267a94f0bf582321b9c61e74 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_2aa4e1a94e432a57feaaa7fc86602141 = $(`&lt;div id=&quot;html_2aa4e1a94e432a57feaaa7fc86602141&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_f426e6eb267a94f0bf582321b9c61e74.setContent(html_2aa4e1a94e432a57feaaa7fc86602141);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_a34573d0858c2d2a6236fa2a32846915.bindPopup(popup_f426e6eb267a94f0bf582321b9c61e74)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_19790daff2caf5f51ced530693ec1dc8 = L.marker(\n",
       "                [32.054138, 34.864418],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_ebcc297f3ff30b42b3382ec7c2b4e310 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_19790daff2caf5f51ced530693ec1dc8.setIcon(icon_ebcc297f3ff30b42b3382ec7c2b4e310);\n",
       "        \n",
       "    \n",
       "        var popup_a239d1cd623c3a8d32597d3c5dbc983b = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_4ef0cef1c6055a46cff98ad76c4a2393 = $(`&lt;div id=&quot;html_4ef0cef1c6055a46cff98ad76c4a2393&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_a239d1cd623c3a8d32597d3c5dbc983b.setContent(html_4ef0cef1c6055a46cff98ad76c4a2393);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_19790daff2caf5f51ced530693ec1dc8.bindPopup(popup_a239d1cd623c3a8d32597d3c5dbc983b)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_d63b4000a12f46d9c5f35af83593d266 = L.marker(\n",
       "                [32.047882, 34.865272],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_644a9fe2c293ae5bf9b57174ca4e0c61 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_d63b4000a12f46d9c5f35af83593d266.setIcon(icon_644a9fe2c293ae5bf9b57174ca4e0c61);\n",
       "        \n",
       "    \n",
       "        var popup_9023ea8969bfac01495e44bb19a8a6cc = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_7240f4468d6ff01dcd753755a537f314 = $(`&lt;div id=&quot;html_7240f4468d6ff01dcd753755a537f314&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_9023ea8969bfac01495e44bb19a8a6cc.setContent(html_7240f4468d6ff01dcd753755a537f314);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_d63b4000a12f46d9c5f35af83593d266.bindPopup(popup_9023ea8969bfac01495e44bb19a8a6cc)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_623549951b8d75a36be2e6db8abb1916 = L.marker(\n",
       "                [32.049659, 34.856235],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_12227ee4cfc76bb6cf5c41027d853e67 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_623549951b8d75a36be2e6db8abb1916.setIcon(icon_12227ee4cfc76bb6cf5c41027d853e67);\n",
       "        \n",
       "    \n",
       "        var popup_ea25f34ea9a3bc035df220dbcaf05d15 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_a3077dfa53023ebccf041b1f6d647d30 = $(`&lt;div id=&quot;html_a3077dfa53023ebccf041b1f6d647d30&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_ea25f34ea9a3bc035df220dbcaf05d15.setContent(html_a3077dfa53023ebccf041b1f6d647d30);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_623549951b8d75a36be2e6db8abb1916.bindPopup(popup_ea25f34ea9a3bc035df220dbcaf05d15)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_21d3224d46010fa5be88a551630e1e60 = L.marker(\n",
       "                [32.047676, 34.862758],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_f1204a56a2e0c34f4e96938244c8e04c = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_21d3224d46010fa5be88a551630e1e60.setIcon(icon_f1204a56a2e0c34f4e96938244c8e04c);\n",
       "        \n",
       "    \n",
       "        var popup_69de902351677e708000e7861790537a = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_588c5e71ad0d96eb29010cf1e2f51776 = $(`&lt;div id=&quot;html_588c5e71ad0d96eb29010cf1e2f51776&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: Yes&lt;/div&gt;`)[0];\n",
       "                popup_69de902351677e708000e7861790537a.setContent(html_588c5e71ad0d96eb29010cf1e2f51776);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_21d3224d46010fa5be88a551630e1e60.bindPopup(popup_69de902351677e708000e7861790537a)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_3fe264241d48e9ca50a17a59397c8084 = L.marker(\n",
       "                [32.045978, 34.865734],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_3d728322757bfaa83a8bc92ea0fb9a8a = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_3fe264241d48e9ca50a17a59397c8084.setIcon(icon_3d728322757bfaa83a8bc92ea0fb9a8a);\n",
       "        \n",
       "    \n",
       "        var popup_ae19bae90f2f93f235b20a22bc435c01 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_fa4dc3cc27fb536552d60aa197cb210a = $(`&lt;div id=&quot;html_fa4dc3cc27fb536552d60aa197cb210a&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Moving Towards Stop: No&lt;/div&gt;`)[0];\n",
       "                popup_ae19bae90f2f93f235b20a22bc435c01.setContent(html_fa4dc3cc27fb536552d60aa197cb210a);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_3fe264241d48e9ca50a17a59397c8084.bindPopup(popup_ae19bae90f2f93f235b20a22bc435c01)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            var marker_5c5f77249b1718f7013f8fd016207551 = L.marker(\n",
       "                [32.082661, 34.845048],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_15c036a476c93efe53eaf4529929208e = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_5c5f77249b1718f7013f8fd016207551.setIcon(icon_15c036a476c93efe53eaf4529929208e);\n",
       "        \n",
       "    \n",
       "            var marker_10e0596062a5dbda487dd2e80acea6d3 = L.marker(\n",
       "                [32.074823, 34.843553],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_e125ddb94916c7e11b77bf6af469aa40 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_10e0596062a5dbda487dd2e80acea6d3.setIcon(icon_e125ddb94916c7e11b77bf6af469aa40);\n",
       "        \n",
       "    \n",
       "            var marker_f48bcc8abf17256400ae61ae26f7e8e9 = L.marker(\n",
       "                [32.067809, 34.83984],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_bc37c77057a3486a8e1c8af09f2f7147 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_f48bcc8abf17256400ae61ae26f7e8e9.setIcon(icon_bc37c77057a3486a8e1c8af09f2f7147);\n",
       "        \n",
       "    \n",
       "            var marker_1127ab88f5dffbf15e736a223f95711a = L.marker(\n",
       "                [32.065377, 34.845842],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_a385a75467181b1a2827f29a9462291e = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_1127ab88f5dffbf15e736a223f95711a.setIcon(icon_a385a75467181b1a2827f29a9462291e);\n",
       "        \n",
       "    \n",
       "            var marker_881533902cd6c7f6a69ccc63c44be21c = L.marker(\n",
       "                [32.065509, 34.850234],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_2b8addf32016e623c9c9bc95e010958f = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_881533902cd6c7f6a69ccc63c44be21c.setIcon(icon_2b8addf32016e623c9c9bc95e010958f);\n",
       "        \n",
       "    \n",
       "            var marker_7bd1fac08b60c80a7c91d283418c9e59 = L.marker(\n",
       "                [32.0654, 34.855732],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_e2a5cd3f43136918fc1975364539a41a = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_7bd1fac08b60c80a7c91d283418c9e59.setIcon(icon_e2a5cd3f43136918fc1975364539a41a);\n",
       "        \n",
       "    \n",
       "            var marker_27fb2b8afffe3633890ebd6a7c146b54 = L.marker(\n",
       "                [32.063529, 34.857014],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_a5440eb3dcf1667d9b8183474c578eba = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_27fb2b8afffe3633890ebd6a7c146b54.setIcon(icon_a5440eb3dcf1667d9b8183474c578eba);\n",
       "        \n",
       "    \n",
       "            var marker_15a33a4a649a49cd0865a539865fa708 = L.marker(\n",
       "                [32.063438, 34.861389],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_9c14cdc35206febd5d51c12ddaf89920 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_15a33a4a649a49cd0865a539865fa708.setIcon(icon_9c14cdc35206febd5d51c12ddaf89920);\n",
       "        \n",
       "    \n",
       "            var marker_f3a0c57cbaeb7d1c36f6df8ec56c57e5 = L.marker(\n",
       "                [32.060705, 34.863046],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_1124b7a1e2c03d3c232a5b8f7bea96a1 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_f3a0c57cbaeb7d1c36f6df8ec56c57e5.setIcon(icon_1124b7a1e2c03d3c232a5b8f7bea96a1);\n",
       "        \n",
       "    \n",
       "            var marker_c787214a5df56af7b375cea21c1355ba = L.marker(\n",
       "                [32.057814, 34.864174],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_4c385a02205dc4c39565e82a13d0ad2d = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_c787214a5df56af7b375cea21c1355ba.setIcon(icon_4c385a02205dc4c39565e82a13d0ad2d);\n",
       "        \n",
       "    \n",
       "            var marker_d5223acb774cbf5b8e2818d4b48976c9 = L.marker(\n",
       "                [32.05514, 34.863882],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_0ce32cd1a153d0adf6bd02c37298b8e8 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_d5223acb774cbf5b8e2818d4b48976c9.setIcon(icon_0ce32cd1a153d0adf6bd02c37298b8e8);\n",
       "        \n",
       "    \n",
       "            var marker_5b4a254fc965d8b7ff4d3a1d0e826029 = L.marker(\n",
       "                [32.04807, 34.86513],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_5f0173cdb353c380b73b7a1e5b7763b9 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_5b4a254fc965d8b7ff4d3a1d0e826029.setIcon(icon_5f0173cdb353c380b73b7a1e5b7763b9);\n",
       "        \n",
       "    \n",
       "            var marker_e5be03cf16ee5fd9b6a178852aa1420a = L.marker(\n",
       "                [32.049525, 34.856072],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_a8f01fac7fe4cdce7a291b31fa80bd88 = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_e5be03cf16ee5fd9b6a178852aa1420a.setIcon(icon_a8f01fac7fe4cdce7a291b31fa80bd88);\n",
       "        \n",
       "    \n",
       "            var marker_a52b31409b946825c7e4721877a29c75 = L.marker(\n",
       "                [32.048301, 34.860287],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_677e100541fb982bb780617da4797c6c = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_a52b31409b946825c7e4721877a29c75.setIcon(icon_677e100541fb982bb780617da4797c6c);\n",
       "        \n",
       "    \n",
       "            var marker_fe22c29fa71e89c6603cb3dd6306f210 = L.marker(\n",
       "                [32.045211, 34.8658],\n",
       "                {}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var icon_2aa0ebbc64394c06f3614577f1bb743d = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;red&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_fe22c29fa71e89c6603cb3dd6306f210.setIcon(icon_2aa0ebbc64394c06f3614577f1bb743d);\n",
       "        \n",
       "    \n",
       "            var poly_line_eb0465b9fdb92d25e15f724764dc4d48 = L.polyline(\n",
       "                [[32.0825, 34.845043], [32.07529, 34.843635], [32.068443, 34.840351], [32.064876, 34.84473], [32.066123, 34.849441], [32.065547, 34.854652], [32.063648, 34.857154], [32.063495, 34.861362], [32.060867, 34.86301], [32.057689, 34.864395], [32.054138, 34.864418], [32.047882, 34.865272], [32.049659, 34.856235], [32.047676, 34.862758], [32.045978, 34.865734]],\n",
       "                {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;blue&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: false, &quot;fillColor&quot;: &quot;blue&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;noClip&quot;: false, &quot;opacity&quot;: 1.0, &quot;smoothFactor&quot;: 1.0, &quot;stroke&quot;: true, &quot;weight&quot;: 3}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "    \n",
       "            var poly_line_59321b515360eac55cdcef3a2c975f2d = L.polyline(\n",
       "                [[32.082661, 34.845048], [32.074823, 34.843553], [32.067809, 34.83984], [32.065377, 34.845842], [32.065509, 34.850234], [32.0654, 34.855732], [32.063529, 34.857014], [32.063438, 34.861389], [32.060705, 34.863046], [32.057814, 34.864174], [32.05514, 34.863882], [32.04807, 34.86513], [32.049525, 34.856072], [32.048301, 34.860287], [32.045211, 34.8658]],\n",
       "                {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;red&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: false, &quot;fillColor&quot;: &quot;red&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;noClip&quot;: false, &quot;opacity&quot;: 1.0, &quot;smoothFactor&quot;: 1.0, &quot;stroke&quot;: true, &quot;weight&quot;: 3}\n",
       "            ).addTo(map_9d2f5dc51a2a2f2e025514c9394e72f9);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x18c241250>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_journey('2023-01-01-215456', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bus Arrival Time Analysis\n",
    "\n",
    "In this Python code snippet, we're performing several key steps to analyze bus arrival times:\n",
    "\n",
    "1. **Calculate Arrival Time Difference**: We first calculate the difference between the estimated arrival time and the scheduled arrival time for each bus. This is done by subtracting the `scheduled_arrival_time` from the `estimated_arrival_time`. The result is stored in a new column in our DataFrame, `results`, named `arrival_time_diff`.\n",
    "\n",
    "2. **Define Bus Classification Function**: We then define a function, `classify_bus`, that takes in the arrival time difference and a threshold (in minutes) as parameters. This function classifies the bus status into one of four categories:\n",
    "\n",
    "   - 'Missing': If the arrival time difference is null.\n",
    "   - 'Late': If the bus arrived later than the scheduled time by more than the threshold.\n",
    "   - 'Early': If the bus arrived earlier than the scheduled time by more than the threshold.\n",
    "   - 'On Time': If the bus arrived within the threshold window.\n",
    "\n",
    "3. **Apply Classification Function**: We set the threshold to 3 minutes and apply the `classify_bus` function to the `arrival_time_diff` column of our DataFrame. The resulting bus status for each row is stored in a new column, `bus_status`.\n",
    "\n",
    "4. **Group and Count Bus Status**: Finally, we group the DataFrame by `siri_journey_ref` (which represents a unique bus journey) and `bus_status`, and count the size of each group. This gives us a count of each bus status category for each unique bus journey. The result is stored in `bus_stats`.\n",
    "\n",
    "This preprocessing step is key as it allows us to categorize bus arrival times and understand the distribution of 'Late', 'Early', 'On Time', and 'Missing' statuses across different bus journeys. This categorized data can then be used for further analysis, such as identifying patterns or trends in bus arrival times, or investigating the factors that might influence a bus's punctuality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def classify_bus(arrival_time_diff, threshold):\n",
    "    if pd.isnull(arrival_time_diff):\n",
    "        return 'Missing'\n",
    "    elif arrival_time_diff > pd.Timedelta(minutes=threshold):\n",
    "        return 'Late'\n",
    "    elif arrival_time_diff < -pd.Timedelta(minutes=threshold):\n",
    "        return 'Early'\n",
    "    else:\n",
    "        return 'On Time'\n",
    "\n",
    "threshold = 3\n",
    "def calculate_bus_stats_and_update_df(results, threshold):\n",
    "    results['bus_status'] = results['arrival_time_diff'].apply(lambda x: classify_bus(x, threshold))\n",
    "    bus_stats = results.groupby(['siri_journey_ref', 'bus_status']).size()\n",
    "    return bus_stats, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stats, results = calculate_bus_stats_and_update_df(results, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Late Bus Stops\n",
    "\n",
    "The Python code snippet performs the following operations:\n",
    "\n",
    "1. **Filter the DataFrame**: The `results` DataFrame is filtered to only include rows where the `bus_status` is 'Late'. This filtered DataFrame, `late_buses`, includes all instances where a bus was late to a stop, regardless of whether it's the same bus or the same journey.\n",
    "\n",
    "2. **Analyze the 'recorded_at_time' column**: The `recorded_at_time` column of the `late_buses` DataFrame is analyzed to understand the distribution of late bus stops throughout the day. This is done by extracting the hour from the `recorded_at_time` and creating a histogram.\n",
    "\n",
    "3. **Plot Histogram**: A histogram is plotted to visualize the number of late bus stops for each hour of the day. The x-axis represents the hour of the day (24-hour format), and the y-axis represents the number of late bus stops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_late_buses(results):\n",
    "    late_buses = results[results['bus_status'] == 'Late']\n",
    "    late_buses['recorded_at_time'].dt.hour.hist()\n",
    "    plt.title('Time of Day for Late Buses')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Late Buses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_late_buses(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Analysis\n",
    "\n",
    "The following Python code block performs several data preprocessing steps on a DataFrame named `results`. Here's a breakdown of what each section does:\n",
    "\n",
    "1. **Bus Status Conversion**: Converts the 'bus_status' column to numerical values, where 'Late' is represented as 1 and other statuses as 0.\n",
    "\n",
    "2. **Time Feature Extraction**: Extracts day of the week and hour from 'gtfs_start_time', 'gtfs_end_time', and 'recorded_at_time' columns.\n",
    "\n",
    "3. **Correlation Calculation**: Calculates the correlation of numerical columns with 'bus_status_num' and sorts them.\n",
    "\n",
    "4. **Pivot Table Creation**: Creates pivot tables for 'recorded_at_time', 'gtfs_start_time', and 'gtfs_end_time' with 'bus_status_num' as values. The pivot tables are indexed by day of the week and hour, and missing values are filled with 0.\n",
    "\n",
    "The pivot tables provide a summary of the average bus status (late or not) for each hour of each day of the week, for the start time, end time, and recorded time. This can be useful for identifying patterns or trends in bus lateness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "outputs": [],
   "source": [
    "def process_results(results):\n",
    "    results['bus_status_num'] = results['bus_status'].apply(lambda x: 1 if x == 'Late' else 0)\n",
    "\n",
    "    results['gtfs_start_time__day_of_week'] = results['gtfs_start_time'].dt.dayofweek\n",
    "    results['gtfs_start_time__hour'] = results['gtfs_start_time'].dt.hour\n",
    "\n",
    "    results['gtfs_end_time__day_of_week'] = results['gtfs_end_time'].dt.dayofweek\n",
    "    results['gtfs_end_time__hour'] = results['gtfs_end_time'].dt.hour\n",
    "\n",
    "    results['recorded_at_time__day_of_week'] = results['recorded_at_time'].dt.dayofweek\n",
    "    results['recorded_at_time__hour'] = results['recorded_at_time'].dt.hour\n",
    "    results['recorded_at_time__minute'] = results['recorded_at_time'].dt.minute\n",
    "    results['recorded_at_time__day_of_month'] = results['recorded_at_time'].dt.day\n",
    "    results['recorded_at_time__month'] = results['recorded_at_time'].dt.month\n",
    "\n",
    "    results.drop(columns=['gtfs_start_time', 'gtfs_end_time', 'recorded_at_time'], inplace=True)\n",
    "\n",
    "    # results['distance_to_prev_stop'] = results.groupby('siri_journey_ref')['distance_from_stop'].diff().fillna(0)\n",
    "    # results['time_diff_to_prev_stop'] = results.groupby('siri_journey_ref')['arrival_time_diff'].diff().fillna(0)\n",
    "    # results['speed_to_prev_stop'] =  np.where(results['time_diff_to_prev_stop'] != 0, results['distance_to_prev_stop'] / results['time_diff_to_prev_stop'], 0)\n",
    "    # results['avg_speed_to_prev_stop'] = results.groupby('journey ref')['speed_to_prev_stop'].transform('mean')\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Visualization\n",
    "\n",
    "The Python code block below generates heatmaps for the pivot tables created in the previous step. The heatmaps visualize the average bus status (late or not) for each hour of each day of the week, for the start time, end time, and recorded time.\n",
    "\n",
    "Three sets of heatmaps are created:\n",
    "\n",
    "1. **Recorded Time Heatmaps**: These heatmaps use the pivot table indexed by the day of the week and hour of the 'recorded_at_time'. Each heatmap represents a day of the week.\n",
    "\n",
    "2. **Start Time Heatmaps**: These heatmaps use the pivot table indexed by the day of the week and hour of the 'gtfs_start_time'. Each heatmap represents a day of the week.\n",
    "\n",
    "3. **End Time Heatmaps**: These heatmaps use the pivot table indexed by the day of the week and hour of the 'gtfs_end_time'. Each heatmap represents a day of the week.\n",
    "\n",
    "The color gradient in the heatmaps represents the average bus status, with lighter colors indicating a higher likelihood of the bus being late. This visualization can help identify patterns or trends in bus lateness across different times and days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def create_pivot_tables(results):\n",
    "    def create_pivot_table(df, index_cols, value_col):\n",
    "        pivot_table = df.pivot_table(index=index_cols, values=value_col, aggfunc='mean')\n",
    "        all_days_of_week = sorted(df[index_cols[0]].unique())\n",
    "        all_hours = range(24)\n",
    "        index = pd.MultiIndex.from_product([all_days_of_week, all_hours], names=index_cols)\n",
    "        pivot_table = pivot_table.reindex(index)\n",
    "        pivot_table.fillna(0, inplace=True)\n",
    "        return pivot_table\n",
    "\n",
    "    recorded_time_pivot = create_pivot_table(results, ['recorded_at_time__day_of_week', 'recorded_at_time__hour'], 'bus_status_num')\n",
    "    start_time_pivot = create_pivot_table(results, ['gtfs_start_time__day_of_week', 'gtfs_start_time__hour'], 'bus_status_num')\n",
    "    end_time_pivot = create_pivot_table(results, ['gtfs_end_time__day_of_week', 'gtfs_end_time__hour'], 'bus_status_num')\n",
    "\n",
    "    return recorded_time_pivot, start_time_pivot, end_time_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_time_pivot, start_time_pivot, end_time_pivot = create_pivot_tables(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_heatmaps(pivot_tables, titles):\n",
    "    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    for pivot_table, title in zip(pivot_tables, titles):\n",
    "        fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "        # Remove the extra subplot\n",
    "        fig.delaxes(axs[1,3])\n",
    "\n",
    "        for i, day in enumerate(days_of_week):\n",
    "            sns.heatmap(pivot_table.loc[i], cmap='viridis', ax=axs[i//4, i%4])\n",
    "            axs[i//4, i%4].set_title(f'Heatmap for {day} - {title}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps([recorded_time_pivot, start_time_pivot, end_time_pivot], ['recorded_at_time', 'gtfs_start_time', 'gtfs_end_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bus Delays by City\n",
    "\n",
    "In this analysis, we are investigating the delay of buses across different cities.\n",
    "\n",
    "The Python code provided creates a pivot table from the `results` DataFrame, using the 'gtfs_stop_city' column as the index and the 'bus_status_num' column as the values. The aggregation function used is 'mean', which gives us the average delay for each city.\n",
    "\n",
    "A function `reverse_string(s)` is defined and applied to the index of the pivot table to reverse the order of the text.\n",
    "\n",
    "A heatmap is then created using seaborn's `sns.heatmap()` function, visualizing the average delay of buses in different cities. The color intensity in the heatmap represents the magnitude of the delay.\n",
    "\n",
    "By examining this heatmap, we can identify which cities experience the most significant bus delays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_city_heatmap(results):\n",
    "    pivot_table_city = results.pivot_table(index='gtfs_stop_city', values='bus_status_num', aggfunc='mean')\n",
    "\n",
    "    def reverse_string(s):\n",
    "        return s[::-1]\n",
    "\n",
    "    pivot_table_city.index = pivot_table_city.index.map(reverse_string)\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot_table_city, annot=True, cmap='coolwarm')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_city_heatmap(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "plot_city_heatmap(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Analysis to the Entire Dataset\n",
    "\n",
    "In this section, we will apply the analysis we have developed to the entire dataset. Given the large size of the data (approximately 800GB), we will process the data in batches. This approach allows us to efficiently handle the data without overloading the memory.\n",
    "\n",
    "The steps we will follow are:\n",
    "\n",
    "1. **Batch Creation**: We will create data batches, ensuring that each batch does not exceed a specified size limit. To maintain data integrity, we will ensure that all related lines (i.e., lines with the same 'siri_journey_ref') are included in the same batch.\n",
    "\n",
    "2. **Batch Analysis**: We will apply our analysis to each batch individually. This includes any data cleaning, transformation, and statistical analysis we have previously defined.\n",
    "\n",
    "3. **Results Aggregation**: After analyzing each batch, we will aggregate the results. This could involve combining the results into a single data structure, or it could involve saving the results of each batch's analysis to disk.\n",
    "\n",
    "By processing the data in batches, we can scale our analysis to handle large datasets that would not otherwise fit into memory. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Creation Function\n",
    "\n",
    "In this cell, we define a function `create_batch_df` to create data batches from the large dataset. The function takes the following parameters:\n",
    "\n",
    "- `csv_dir`: The directory where the CSV files are stored.\n",
    "- `batch_size_gb`: The maximum size of each batch in gigabytes.\n",
    "- `last_journey_refs`: A set of 'siri_journey_ref' values that were included in the last batch and should be excluded from the current batch.\n",
    "- `start_file`: The file to start reading from. If not specified, reading starts from the first file in the directory.\n",
    "- `start_pos`: The position in the start file to start reading from.\n",
    "\n",
    "The function works as follows:\n",
    "\n",
    "1. It initializes an empty DataFrame for the batch and a set to keep track of the 'siri_journey_ref' values in the current batch.\n",
    "\n",
    "2. It iterates over each file in the directory, starting from the `start_file` if specified.\n",
    "\n",
    "3. For each file, it creates an iterator for the chunks in the file and reads the first chunk.\n",
    "\n",
    "4. If there are any `last_journey_refs`, it excludes them from the chunk.\n",
    "\n",
    "5. It then enters a loop where it adds the chunk to the batch if it doesn't exceed the `batch_size_gb`. If the batch size is exceeded, it adds only the rows with the same 'siri_journey_ref' as in the current batch.\n",
    "\n",
    "6. The function returns the batch DataFrame, the set of 'siri_journey_ref' values in the current batch, the next chunk, the iterator for the next chunks, the next file, and the index of the next file.\n",
    "\n",
    "By using this function, we can create manageable batches from the large dataset while ensuring that all related lines are included in the same batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import uuid\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def create_batch_df(parquet_dir, batch_size_gb, last_batch_journey_refs=None, start_file=None, start_pos=0):\n",
    "    # Convert batch size from GB to bytes\n",
    "    batch_size_bytes = batch_size_gb * 1024 * 1024 * 1024\n",
    "\n",
    "    # Get a list of all output Parquet files\n",
    "    parquet_files = glob.glob(f'{parquet_dir}/output_*.parquet')\n",
    "    parquet_files.sort(key=extract_number)\n",
    "\n",
    "    # If a start file is specified, start from this file\n",
    "    if start_file is not None:\n",
    "        parquet_files = parquet_files[parquet_files.index(start_file):]\n",
    "\n",
    "    # Initialize an empty DataFrame for the batch\n",
    "    batch_df = pd.DataFrame()\n",
    "\n",
    "    # Initialize a set to keep track of the 'siri_journey_ref' values in the current batch\n",
    "    current_journey_refs = set()\n",
    "\n",
    "    for file in parquet_files:\n",
    "        # Create an iterator for the chunks in the Parquet file\n",
    "        parquet_file = pq.ParquetFile(file)\n",
    "        chunk_iter = parquet_file.iter_batches(batch_size=100000)  # Adjust batch size as needed\n",
    "\n",
    "        # Read the first chunk and convert it to a DataFrame\n",
    "        chunk = pd.DataFrame(next(chunk_iter).to_pandas())\n",
    "\n",
    "        # If there are any last journey refs, exclude them from the chunk\n",
    "        if last_batch_journey_refs is not None:\n",
    "            chunk = chunk[~chunk['siri_journey_ref'].isin(last_batch_journey_refs)]\n",
    "        while True:\n",
    "            # Add the chunk to the batch if it doesn't exceed the batch size\n",
    "            if (batch_df.memory_usage(index=True, deep=True).sum() + chunk.memory_usage(index=True, deep=True).sum()) <= batch_size_bytes:\n",
    "                batch_df = pd.concat([batch_df, chunk])\n",
    "                current_journey_refs.update(chunk['siri_journey_ref'].unique())\n",
    "\n",
    "            else:\n",
    "                # If the batch size is exceeded, add only the rows with the same 'siri_journey_ref' as in the current batch\n",
    "                current_journey_refs_in_chunk = chunk['siri_journey_ref'].isin(current_journey_refs)\n",
    "                if current_journey_refs_in_chunk.any():\n",
    "                    batch_df = pd.concat([batch_df, chunk[current_journey_refs_in_chunk]])\n",
    "                    chunk = chunk[~current_journey_refs_in_chunk]\n",
    "\n",
    "                # Return the current DataFrame and the set of 'siri_journey_ref' values\n",
    "                return batch_df, current_journey_refs, chunk, chunk_iter, file, parquet_files.index(file)\n",
    "\n",
    "            # Try to read the next chunk\n",
    "            try:\n",
    "                chunk = pd.DataFrame(next(chunk_iter).to_pandas())\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    # If all files have been processed, return the current DataFrame and the set of 'siri_journey_ref' values\n",
    "    return batch_df, current_journey_refs, None, None, None, None\n",
    "\n",
    "def process_batch_df(batch_df):\n",
    "    # Process the DataFrame\n",
    "    df_bus_journey_stops, df_locations = process_dataframe(batch_df)\n",
    "\n",
    "    print(f'Processed {len(df_bus_journey_stops)} bus journey stops and {len(df_locations)} locations.')\n",
    "    # Process the locations\n",
    "    df_locations = process_locations(df_locations)\n",
    "\n",
    "    # Calculate the arrival times\n",
    "    results = calculate_arrival_times(df_locations, df_bus_journey_stops, bearing_threshold=30)\n",
    "\n",
    "    print(f'Calculated arrival times for {len(results)} stops.')\n",
    "    # Calculate the bus stats and update the DataFrame\n",
    "    _, results = calculate_bus_stats_and_update_df(results, threshold=3)\n",
    "\n",
    "    # Process the results\n",
    "    results = process_results(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_processed_batch(batch_df):\n",
    "    batch_num = uuid.uuid4()\n",
    "    output_file = f'{DATA_FOLDER}/processed_batches/processed_batch_{batch_num}.parquet'\n",
    "    print(f'Saving processed batch to {output_file}')\n",
    "    batch_df.to_parquet(output_file, index=False, engine='fastparquet')    \n",
    "\n",
    "def process_batches_parallel(batch_dfs):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_batch_df, batch_dfs))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def process_parquet_files(parquet_dir, batch_size_gb, num_batches):\n",
    "    last_batch_journey_refs = None\n",
    "    start_file = None\n",
    "    start_pos = 0\n",
    "\n",
    "    while True:\n",
    "        batches = []\n",
    "        for _ in range(num_batches):\n",
    "            batch_df, last_batch_journey_refs, remaining_chunk, chunk_iter, start_file, start_pos = create_batch_df(parquet_dir, batch_size_gb, last_batch_journey_refs, start_file, start_pos)\n",
    "            batches.append(batch_df)\n",
    "\n",
    "            # If there are no more files to process, break the loop\n",
    "            if start_file is None:\n",
    "                break\n",
    "\n",
    "        if not batches:\n",
    "            break\n",
    "\n",
    "        processes_batches =  process_batches_parallel(batches)\n",
    "\n",
    "        for batch in processes_batches:\n",
    "            save_processed_batch(batch)\n",
    "\n",
    "        batches.clear()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df, _, _, _, _, _ = create_batch_df(f'{DATA_FOLDER}/concatenated_data_parquet', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_parquet_files(f'{DATA_FOLDER}/concatenated_data_parquet', 0.4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where your Parquet files are located\n",
    "parquet_dir = f'{DATA_FOLDER}/concatenated_data_parquet'\n",
    "\n",
    "# Create a batch of 1GB\n",
    "batch_df, current_journey_refs, chunk, chunk_iter, file, file_index = create_batch_df(parquet_dir, batch_size_gb=0.2)\n",
    "\n",
    "print(f'Batch DataFrame shape: {batch_df.shape}')\n",
    "\n",
    "print(f'Batch size in GB: {batch_df.memory_usage(index=True, deep=True).sum() / 1024**3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "def profile_process_batch_df():\n",
    "\n",
    "    # Use cProfile to profile the function\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    # Call the function you want to profile\n",
    "    process_batch_df(batch_df)\n",
    "\n",
    "    # Disable the profiler after your function call\n",
    "    profiler.disable()\n",
    "\n",
    "    # Print the stats\n",
    "    profiler.print_stats()\n",
    "\n",
    "# Call the profiling function\n",
    "profile_process_batch_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_data = batch.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "bus_data['siri_journey_ref'] = label_encoder.fit_transform(bus_data['siri_journey_ref'])\n",
    "# Dropping string columns\n",
    "bus_data.drop(columns=['gtfs_route_short_name', 'gtfs_journey_ref', 'gtfs_arrival_time'], inplace=True) \n",
    "# Dropping null data\n",
    "bus_data.dropna(inplace=True)\n",
    "# formatting time diff to sec from TimeDelta\n",
    "bus_data['arrival_time_diff'] = bus_data['arrival_time_diff'].dt.total_seconds()\n",
    "\n",
    "bus_data['city_encoded'] = label_encoder.fit_transform(bus_data['gtfs_stop_city'])\n",
    "\n",
    "bus_data = pd.get_dummies(bus_data, columns=['bus_status'])\n",
    "bus_data.drop(columns=['bus_status_num'], inplace=True)\n",
    "\n",
    "time_columns = [col for col in bus_data.columns if col.endswith('_time')]\n",
    "\n",
    "for col in time_columns:\n",
    "    # Convert the column to datetime\n",
    "    bus_data[col] = pd.to_datetime(bus_data[col])\n",
    "    \n",
    "    # Create new columns for month, day of month, hour and minute\n",
    "    bus_data[col + '__month'] = bus_data[col].dt.month\n",
    "    bus_data[col + '__day'] = bus_data[col].dt.day\n",
    "    bus_data[col + '__day_of_week'] = bus_data[col].dt.dayofweek\n",
    "    bus_data[col + '__hour'] = bus_data[col].dt.hour\n",
    "    bus_data[col + '__minute'] = bus_data[col].dt.minute\n",
    "\n",
    "bus_data.drop(columns=time_columns, inplace=True)\n",
    "\n",
    "id_features = ['id', 'siri_journey_ref', 'siri_vehicle_ref', 'siri_stop_code', 'siri_operator_ref', 'siri_line_ref']\n",
    "\n",
    "other_features = ['gtfs_stop_sequence', 'siri_stop_order', 'gtfs_route_direction',\n",
    "                   'closet_location_speed', 'closest_location_bearing', 'distance_from_stop',\n",
    "                     'moving_towards_stop', 'city_encoded']\n",
    "time_features = ['gtfs_start_time__day_of_week', 'gtfs_start_time__hour',\n",
    "       'gtfs_end_time__day_of_week', 'gtfs_end_time__hour',\n",
    "       'recorded_at_time__day_of_week', 'recorded_at_time__hour',\n",
    "       'recorded_at_time__minute', 'recorded_at_time__day_of_month',\n",
    "       'recorded_at_time__month', 'scheduled_arrival_time__month', 'scheduled_arrival_time__day',\n",
    "       'scheduled_arrival_time__day_of_week', 'scheduled_arrival_time__hour',\n",
    "       'scheduled_arrival_time__minute']\n",
    "\n",
    "target_variables = ['bus_status_Early', 'bus_status_Late', 'bus_status_On Time', 'arrival_time_diff']\n",
    "target_variables_time = ['estimated_arrival_time__month',\n",
    "       'estimated_arrival_time__day', 'estimated_arrival_time__day_of_week',\n",
    "       'estimated_arrival_time__hour', 'estimated_arrival_time__minute']\n",
    "\n",
    "features = id_features + other_features + time_features \n",
    "target = target_variables + target_variables_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>siri_journey_ref</th>\n",
       "      <th>siri_vehicle_ref</th>\n",
       "      <th>siri_stop_code</th>\n",
       "      <th>siri_operator_ref</th>\n",
       "      <th>siri_line_ref</th>\n",
       "      <th>gtfs_stop_sequence</th>\n",
       "      <th>siri_stop_order</th>\n",
       "      <th>gtfs_route_direction</th>\n",
       "      <th>closet_location_speed</th>\n",
       "      <th>...</th>\n",
       "      <th>recorded_at_time__day_of_week</th>\n",
       "      <th>recorded_at_time__hour</th>\n",
       "      <th>recorded_at_time__minute</th>\n",
       "      <th>recorded_at_time__day_of_month</th>\n",
       "      <th>recorded_at_time__month</th>\n",
       "      <th>scheduled_arrival_time__month</th>\n",
       "      <th>scheduled_arrival_time__day</th>\n",
       "      <th>scheduled_arrival_time__day_of_week</th>\n",
       "      <th>scheduled_arrival_time__hour</th>\n",
       "      <th>scheduled_arrival_time__minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1473611409</td>\n",
       "      <td>0</td>\n",
       "      <td>7728469</td>\n",
       "      <td>2524</td>\n",
       "      <td>3</td>\n",
       "      <td>12390</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.474103e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1473609384</td>\n",
       "      <td>0</td>\n",
       "      <td>7317852</td>\n",
       "      <td>6071</td>\n",
       "      <td>50</td>\n",
       "      <td>15138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.286680e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1473609384</td>\n",
       "      <td>0</td>\n",
       "      <td>7317852</td>\n",
       "      <td>6071</td>\n",
       "      <td>50</td>\n",
       "      <td>15138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.286680e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1473609384</td>\n",
       "      <td>0</td>\n",
       "      <td>7317852</td>\n",
       "      <td>6071</td>\n",
       "      <td>50</td>\n",
       "      <td>15138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.286680e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1473609384</td>\n",
       "      <td>0</td>\n",
       "      <td>7317852</td>\n",
       "      <td>6071</td>\n",
       "      <td>50</td>\n",
       "      <td>15138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.286680e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429073</th>\n",
       "      <td>1473908971</td>\n",
       "      <td>2326</td>\n",
       "      <td>7728669</td>\n",
       "      <td>5985</td>\n",
       "      <td>3</td>\n",
       "      <td>11108</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>6.744721e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429074</th>\n",
       "      <td>1473914652</td>\n",
       "      <td>2326</td>\n",
       "      <td>7728669</td>\n",
       "      <td>2811</td>\n",
       "      <td>3</td>\n",
       "      <td>11108</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>1.018009e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429075</th>\n",
       "      <td>1473920304</td>\n",
       "      <td>2326</td>\n",
       "      <td>7728669</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>11108</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>6.962211e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429076</th>\n",
       "      <td>1473925951</td>\n",
       "      <td>2326</td>\n",
       "      <td>7728669</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>11108</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>6.842899e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429077</th>\n",
       "      <td>1473931604</td>\n",
       "      <td>2326</td>\n",
       "      <td>7728669</td>\n",
       "      <td>3859</td>\n",
       "      <td>3</td>\n",
       "      <td>11108</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>7.883946e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383285 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  siri_journey_ref siri_vehicle_ref siri_stop_code  \\\n",
       "0       1473611409                 0          7728469           2524   \n",
       "17      1473609384                 0          7317852           6071   \n",
       "18      1473609384                 0          7317852           6071   \n",
       "19      1473609384                 0          7317852           6071   \n",
       "20      1473609384                 0          7317852           6071   \n",
       "...            ...               ...              ...            ...   \n",
       "429073  1473908971              2326          7728669           5985   \n",
       "429074  1473914652              2326          7728669           2811   \n",
       "429075  1473920304              2326          7728669            109   \n",
       "429076  1473925951              2326          7728669            112   \n",
       "429077  1473931604              2326          7728669           3859   \n",
       "\n",
       "       siri_operator_ref siri_line_ref gtfs_stop_sequence siri_stop_order  \\\n",
       "0                      3         12390                  1               1   \n",
       "17                    50         15138                  1               1   \n",
       "18                    50         15138                  1               1   \n",
       "19                    50         15138                  1               1   \n",
       "20                    50         15138                  1               1   \n",
       "...                  ...           ...                ...             ...   \n",
       "429073                 3         11108                 33              33   \n",
       "429074                 3         11108                 34              34   \n",
       "429075                 3         11108                 35              35   \n",
       "429076                 3         11108                 36              36   \n",
       "429077                 3         11108                 37              37   \n",
       "\n",
       "       gtfs_route_direction  closet_location_speed  ...  \\\n",
       "0                         2           2.474103e+06  ...   \n",
       "17                        1           1.286680e+06  ...   \n",
       "18                        1           1.286680e+06  ...   \n",
       "19                        1           1.286680e+06  ...   \n",
       "20                        1           1.286680e+06  ...   \n",
       "...                     ...                    ...  ...   \n",
       "429073                    2           6.744721e+01  ...   \n",
       "429074                    2           1.018009e+02  ...   \n",
       "429075                    2           6.962211e+01  ...   \n",
       "429076                    2           6.842899e+01  ...   \n",
       "429077                    2           7.883946e+01  ...   \n",
       "\n",
       "        recorded_at_time__day_of_week  recorded_at_time__hour  \\\n",
       "0                                   3                      12   \n",
       "17                                  3                      12   \n",
       "18                                  3                      12   \n",
       "19                                  3                      12   \n",
       "20                                  3                      12   \n",
       "...                               ...                     ...   \n",
       "429073                              3                      12   \n",
       "429074                              3                      12   \n",
       "429075                              3                      12   \n",
       "429076                              3                      12   \n",
       "429077                              3                      12   \n",
       "\n",
       "        recorded_at_time__minute  recorded_at_time__day_of_month  \\\n",
       "0                             26                               5   \n",
       "17                             0                               5   \n",
       "18                             0                               5   \n",
       "19                             0                               5   \n",
       "20                             0                               5   \n",
       "...                          ...                             ...   \n",
       "429073                        55                               5   \n",
       "429074                        56                               5   \n",
       "429075                        57                               5   \n",
       "429076                        58                               5   \n",
       "429077                        59                               5   \n",
       "\n",
       "        recorded_at_time__month  scheduled_arrival_time__month  \\\n",
       "0                             1                              1   \n",
       "17                            1                              1   \n",
       "18                            1                              1   \n",
       "19                            1                              1   \n",
       "20                            1                              1   \n",
       "...                         ...                            ...   \n",
       "429073                        1                              1   \n",
       "429074                        1                              1   \n",
       "429075                        1                              1   \n",
       "429076                        1                              1   \n",
       "429077                        1                              1   \n",
       "\n",
       "        scheduled_arrival_time__day  scheduled_arrival_time__day_of_week  \\\n",
       "0                                 5                                    3   \n",
       "17                                5                                    3   \n",
       "18                                5                                    3   \n",
       "19                                5                                    3   \n",
       "20                                5                                    3   \n",
       "...                             ...                                  ...   \n",
       "429073                            5                                    3   \n",
       "429074                            5                                    3   \n",
       "429075                            5                                    3   \n",
       "429076                            5                                    3   \n",
       "429077                            5                                    3   \n",
       "\n",
       "        scheduled_arrival_time__hour  scheduled_arrival_time__minute  \n",
       "0                                 12                               3  \n",
       "17                                12                               0  \n",
       "18                                12                               0  \n",
       "19                                12                               0  \n",
       "20                                12                               0  \n",
       "...                              ...                             ...  \n",
       "429073                            12                              41  \n",
       "429074                            12                              42  \n",
       "429075                            12                              43  \n",
       "429076                            12                              43  \n",
       "429077                            12                              44  \n",
       "\n",
       "[383285 rows x 28 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bus_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mce\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m bus_data[features]\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m bus_data[target]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Binary encode categorical features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bus_data' is not defined"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "X = bus_data[features]\n",
    "y = bus_data[target]\n",
    "\n",
    "# Binary encode categorical features\n",
    "encoder = ce.BinaryEncoder(cols=id_features)\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('after scaler')\n",
    "\n",
    "# Define RNN model\n",
    "model = Sequential([\n",
    "    LSTM(units=64, input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(target), activation='linear')\n",
    "])\n",
    "\n",
    "print('after model')\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print('after compile')\n",
    "\n",
    "# Reshape input data for LSTM layer\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print('after fit')\n",
    "\n",
    "# Evaluate model\n",
    "loss = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "batch['siri_journey_ref'] = label_encoder.fit_transform(batch['siri_journey_ref'])\n",
    "batch.drop(columns=['gtfs_stop_city', 'gtfs_route_short_name', 'gtfs_journey_ref', 'gtfs_arrival_time'], inplace=True)\n",
    "batch.dropna(subset=['scheduled_arrival_time'], inplace=True)\n",
    "batch['arrival_time_diff'] = batch['arrival_time_diff'].dt.total_seconds()\n",
    "\n",
    "\n",
    "time_columns = [col for col in batch.columns if col.endswith('_time')]\n",
    "\n",
    "for col in time_columns:\n",
    "    # Convert the column to datetime\n",
    "    batch[col] = pd.to_datetime(batch[col])\n",
    "    \n",
    "    # Create new columns for month, day of month, hour and minute\n",
    "    batch[col + '_month'] = batch[col].dt.month\n",
    "    batch[col + '_day'] = batch[col].dt.day\n",
    "    batch[col + '_hour'] = batch[col].dt.hour\n",
    "    batch[col + '_minute'] = batch[col].dt.minute\n",
    "\n",
    "batch.drop(columns=time_columns, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (447810, 34) (447810, 10)\n",
      "Testing set shape: (298540, 34) (298540, 10)\n"
     ]
    }
   ],
   "source": [
    "batch.dropna(inplace=True)\n",
    "\n",
    "target_variables = ['estimated_arrival_time_month', 'estimated_arrival_time_day', 'estimated_arrival_time_hour', 'estimated_arrival_time_minute', 'arrival_time_diff', 'bus_status_Early', 'bus_status_Late',\n",
    "       'bus_status_Missing', 'bus_status_On Time', 'bus_status_num']\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = batch.drop(columns=target_variables, axis=1)  # Features\n",
    "y = batch[target_variables]  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'siri_journey_ref', 'siri_vehicle_ref', 'siri_stop_code',\n",
       "       'siri_stop_order', 'siri_operator_ref', 'siri_line_ref',\n",
       "       'gtfs_stop_code', 'gtfs_stop_lat', 'gtfs_stop_lon',\n",
       "       'gtfs_stop_sequence', 'gtfs_route_direction', 'gtfs_route_mkt',\n",
       "       'closest_location', 'closest_location_lat', 'closest_location_lon',\n",
       "       'closet_location_speed', 'closest_location_bearing',\n",
       "       'distance_from_stop', 'moving_towards_stop',\n",
       "       'gtfs_start_time__day_of_week', 'gtfs_start_time__hour',\n",
       "       'gtfs_end_time__day_of_week', 'gtfs_end_time__hour',\n",
       "       'recorded_at_time__day_of_week', 'recorded_at_time__hour',\n",
       "       'recorded_at_time__minute', 'day_of_month', 'month', 'city_encoded',\n",
       "       'scheduled_arrival_time_month', 'scheduled_arrival_time_day',\n",
       "       'scheduled_arrival_time_hour', 'scheduled_arrival_time_minute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns\n",
    "# print(X_test.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.91045040e-02 -1.42634560e-02  7.03054681e-03  2.53367078e-02\n",
      "  -5.86579420e-03  1.54269521e-02  1.03377860e-02  2.53367078e-02\n",
      "   9.66390899e-01 -8.49031108e-02 -3.37567522e-02  3.50223004e-02\n",
      "  -2.56669433e-02 -4.15472763e-02 -1.00335367e+00  9.87687343e-02\n",
      "  -6.00040732e-04 -7.21814844e-03 -9.32787515e-03  1.77349205e-02\n",
      "   2.78683326e-15 -1.23549354e-02 -2.83106871e-15 -3.42954919e-02\n",
      "  -1.88737914e-15  1.79534296e-03  1.47625921e-02  0.00000000e+00\n",
      "   0.00000000e+00  5.98828325e-03  0.00000000e+00  0.00000000e+00\n",
      "   3.42785957e-02  4.88478825e-02]\n",
      " [-1.33059482e-01  6.17440548e-02 -6.35139390e-02  4.39416712e-02\n",
      "   1.62164336e-01  1.37880524e-02  3.55389588e-02  4.39416712e-02\n",
      "   9.55932158e+00  3.25967782e+00 -1.26473874e-01  2.18336987e-02\n",
      "   1.58588061e-02  4.43471957e-01 -9.61019421e+00 -3.26499520e+00\n",
      "  -2.52326223e-02 -9.36975272e-03  4.23082506e-01  8.23390793e-02\n",
      "   2.25514052e-14  2.69746758e-01 -2.84217094e-14 -7.56287591e-02\n",
      "  -1.77635684e-14 -1.35481911e-03  1.44535028e+01  0.00000000e+00\n",
      "   0.00000000e+00  1.08992862e-01  0.00000000e+00  0.00000000e+00\n",
      "  -2.93335400e-02  2.09310740e-01]\n",
      " [ 9.40142742e+01 -4.74100156e+01  2.10321977e+01  9.42357034e+01\n",
      "  -1.42621575e+01  5.89051384e+01  3.77118831e+01  9.42357034e+01\n",
      "   3.90975109e+03 -2.02655128e+02 -1.31993591e+02  1.28079606e+02\n",
      "  -8.99940837e+01 -1.21544320e+02 -4.04600355e+03  2.51074116e+02\n",
      "  -3.73321153e+00 -2.64889046e+01 -8.17977165e+00  7.10856517e+01\n",
      "   1.21929133e-11 -2.74474241e+01 -1.18234311e-11 -1.28810619e+02\n",
      "  -7.27595761e-12  6.32882723e+00  9.18556018e+02  0.00000000e+00\n",
      "   0.00000000e+00  2.79886831e+01  0.00000000e+00  0.00000000e+00\n",
      "  -5.69952023e+02 -6.96785675e+02]\n",
      " [-1.90613829e-02 -2.14431261e-03 -7.09315166e-03  1.67319446e-03\n",
      "  -3.47971201e-02  1.83360505e-02 -1.03430211e-03  1.67319446e-03\n",
      "   6.15704912e-01  1.27218298e+00 -3.38915068e-05  1.99345953e-02\n",
      "   1.42528265e-02  1.65804791e-01 -6.24442635e-01 -1.27007856e+00\n",
      "  -2.46931497e-03 -5.85810140e-03  9.42539216e-03 -2.26787722e-02\n",
      "   8.18789481e-16 -4.21547856e-02 -2.33146835e-15 -9.42962390e-03\n",
      "  -1.44328993e-15 -1.59309533e-05 -4.78758907e-01  0.00000000e+00\n",
      "   0.00000000e+00  4.83175220e-03  0.00000000e+00  0.00000000e+00\n",
      "   2.70286815e-01  3.67558881e-01]\n",
      " [ 7.44467674e-02 -3.85379907e-03  4.71443396e-03 -9.54747627e-03\n",
      "  -5.12764412e-02 -2.36900612e-02 -9.79802101e-03 -9.54747627e-03\n",
      "  -1.55688941e+00 -9.59769089e-01  9.56824985e-02 -8.71931842e-03\n",
      "  -1.64486737e-02 -5.02884961e-03  1.56768143e+00  9.69054921e-01\n",
      "   1.03197066e-02 -8.94058767e-03  2.86065617e-02  3.55951898e-02\n",
      "  -3.51108032e-15 -1.49910635e-02  4.88498131e-15 -3.00168046e-02\n",
      "   3.10862447e-15 -1.02585673e-03  4.89822013e-01  0.00000000e+00\n",
      "   0.00000000e+00  1.18367544e-02  0.00000000e+00  0.00000000e+00\n",
      "  -3.46506942e-01 -4.81374555e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-5.53853846e-02  5.99811168e-03  2.37871770e-03  7.87428182e-03\n",
      "   8.60735613e-02  5.35401071e-03  1.08323231e-02  7.87428182e-03\n",
      "   9.41184500e-01 -3.12413893e-01 -9.56486070e-02 -1.12152768e-02\n",
      "   2.19584724e-03 -1.60775942e-01 -9.43238791e-01  3.01023636e-01\n",
      "  -7.85039167e-03  1.47986891e-02 -3.80319539e-02 -1.29164176e-02\n",
      "   3.12857379e-15  5.71458490e-02 -2.66453526e-15  3.94464285e-02\n",
      "  -1.77635684e-15  1.04178768e-03 -1.10631060e-02  0.00000000e+00\n",
      "   0.00000000e+00 -1.66685066e-02  0.00000000e+00  0.00000000e+00\n",
      "   7.62201266e-02  1.13815674e-01]\n",
      " [ 7.44467674e-02 -3.85379907e-03  4.71443396e-03 -9.54747627e-03\n",
      "  -5.12764412e-02 -2.36900612e-02 -9.79802101e-03 -9.54747627e-03\n",
      "  -1.55688941e+00 -9.59769089e-01  9.56824985e-02 -8.71931842e-03\n",
      "  -1.64486737e-02 -5.02884961e-03  1.56768143e+00  9.69054921e-01\n",
      "   1.03197066e-02 -8.94058767e-03  2.86065617e-02  3.55951898e-02\n",
      "  -3.55271368e-15 -1.49910635e-02  4.88498131e-15 -3.00168046e-02\n",
      "   3.10862447e-15 -1.02585673e-03  4.89822013e-01  0.00000000e+00\n",
      "   0.00000000e+00  1.18367544e-02  0.00000000e+00  0.00000000e+00\n",
      "  -3.46506942e-01 -4.81374555e-01]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m coefficients_flat \u001b[38;5;241m=\u001b[39m coefficients\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to display the coefficients with their corresponding feature names\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoefficient\u001b[39m\u001b[38;5;124m'\u001b[39m: coefficients_flat})\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importance)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Access the coefficients of the linear regression model\n",
    "coefficients = model.coef_\n",
    "print(coefficients)\n",
    "coefficients_flat = coefficients.flatten()\n",
    "\n",
    "# Create a DataFrame to display the coefficients with their corresponding feature names\n",
    "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients_flat})\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 30.69298842014145\n",
      "Mean Squared Error: 102854.20635007213\n",
      "Root Mean Squared Error: 320.7089121774949\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.91045040e-02 -1.42634560e-02  7.03054681e-03  2.53367078e-02\n",
      "  -5.86579420e-03  1.54269521e-02  1.03377860e-02  2.53367078e-02\n",
      "   9.66390899e-01 -8.49031108e-02 -3.37567522e-02  3.50223004e-02\n",
      "  -2.56669433e-02 -4.15472763e-02 -1.00335367e+00  9.87687343e-02\n",
      "  -6.00040732e-04 -7.21814844e-03 -9.32787515e-03  1.77349205e-02\n",
      "   2.78683326e-15 -1.23549354e-02 -2.83106871e-15 -3.42954919e-02\n",
      "  -1.88737914e-15  1.79534296e-03  1.47625921e-02  0.00000000e+00\n",
      "   0.00000000e+00  5.98828325e-03  0.00000000e+00  0.00000000e+00\n",
      "   3.42785957e-02  4.88478825e-02]\n",
      " [-1.33059482e-01  6.17440548e-02 -6.35139390e-02  4.39416712e-02\n",
      "   1.62164336e-01  1.37880524e-02  3.55389588e-02  4.39416712e-02\n",
      "   9.55932158e+00  3.25967782e+00 -1.26473874e-01  2.18336987e-02\n",
      "   1.58588061e-02  4.43471957e-01 -9.61019421e+00 -3.26499520e+00\n",
      "  -2.52326223e-02 -9.36975272e-03  4.23082506e-01  8.23390793e-02\n",
      "   2.25514052e-14  2.69746758e-01 -2.84217094e-14 -7.56287591e-02\n",
      "  -1.77635684e-14 -1.35481911e-03  1.44535028e+01  0.00000000e+00\n",
      "   0.00000000e+00  1.08992862e-01  0.00000000e+00  0.00000000e+00\n",
      "  -2.93335400e-02  2.09310740e-01]\n",
      " [ 9.40142742e+01 -4.74100156e+01  2.10321977e+01  9.42357034e+01\n",
      "  -1.42621575e+01  5.89051384e+01  3.77118831e+01  9.42357034e+01\n",
      "   3.90975109e+03 -2.02655128e+02 -1.31993591e+02  1.28079606e+02\n",
      "  -8.99940837e+01 -1.21544320e+02 -4.04600355e+03  2.51074116e+02\n",
      "  -3.73321153e+00 -2.64889046e+01 -8.17977165e+00  7.10856517e+01\n",
      "   1.21929133e-11 -2.74474241e+01 -1.18234311e-11 -1.28810619e+02\n",
      "  -7.27595761e-12  6.32882723e+00  9.18556018e+02  0.00000000e+00\n",
      "   0.00000000e+00  2.79886831e+01  0.00000000e+00  0.00000000e+00\n",
      "  -5.69952023e+02 -6.96785675e+02]\n",
      " [-1.90613829e-02 -2.14431261e-03 -7.09315166e-03  1.67319446e-03\n",
      "  -3.47971201e-02  1.83360505e-02 -1.03430211e-03  1.67319446e-03\n",
      "   6.15704912e-01  1.27218298e+00 -3.38915068e-05  1.99345953e-02\n",
      "   1.42528265e-02  1.65804791e-01 -6.24442635e-01 -1.27007856e+00\n",
      "  -2.46931497e-03 -5.85810140e-03  9.42539216e-03 -2.26787722e-02\n",
      "   8.18789481e-16 -4.21547856e-02 -2.33146835e-15 -9.42962390e-03\n",
      "  -1.44328993e-15 -1.59309533e-05 -4.78758907e-01  0.00000000e+00\n",
      "   0.00000000e+00  4.83175220e-03  0.00000000e+00  0.00000000e+00\n",
      "   2.70286815e-01  3.67558881e-01]\n",
      " [ 7.44467674e-02 -3.85379907e-03  4.71443396e-03 -9.54747627e-03\n",
      "  -5.12764412e-02 -2.36900612e-02 -9.79802101e-03 -9.54747627e-03\n",
      "  -1.55688941e+00 -9.59769089e-01  9.56824985e-02 -8.71931842e-03\n",
      "  -1.64486737e-02 -5.02884961e-03  1.56768143e+00  9.69054921e-01\n",
      "   1.03197066e-02 -8.94058767e-03  2.86065617e-02  3.55951898e-02\n",
      "  -3.51108032e-15 -1.49910635e-02  4.88498131e-15 -3.00168046e-02\n",
      "   3.10862447e-15 -1.02585673e-03  4.89822013e-01  0.00000000e+00\n",
      "   0.00000000e+00  1.18367544e-02  0.00000000e+00  0.00000000e+00\n",
      "  -3.46506942e-01 -4.81374555e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-5.53853846e-02  5.99811168e-03  2.37871770e-03  7.87428182e-03\n",
      "   8.60735613e-02  5.35401071e-03  1.08323231e-02  7.87428182e-03\n",
      "   9.41184500e-01 -3.12413893e-01 -9.56486070e-02 -1.12152768e-02\n",
      "   2.19584724e-03 -1.60775942e-01 -9.43238791e-01  3.01023636e-01\n",
      "  -7.85039167e-03  1.47986891e-02 -3.80319539e-02 -1.29164176e-02\n",
      "   3.12857379e-15  5.71458490e-02 -2.66453526e-15  3.94464285e-02\n",
      "  -1.77635684e-15  1.04178768e-03 -1.10631060e-02  0.00000000e+00\n",
      "   0.00000000e+00 -1.66685066e-02  0.00000000e+00  0.00000000e+00\n",
      "   7.62201266e-02  1.13815674e-01]\n",
      " [ 7.44467674e-02 -3.85379907e-03  4.71443396e-03 -9.54747627e-03\n",
      "  -5.12764412e-02 -2.36900612e-02 -9.79802101e-03 -9.54747627e-03\n",
      "  -1.55688941e+00 -9.59769089e-01  9.56824985e-02 -8.71931842e-03\n",
      "  -1.64486737e-02 -5.02884961e-03  1.56768143e+00  9.69054921e-01\n",
      "   1.03197066e-02 -8.94058767e-03  2.86065617e-02  3.55951898e-02\n",
      "  -3.55271368e-15 -1.49910635e-02  4.88498131e-15 -3.00168046e-02\n",
      "   3.10862447e-15 -1.02585673e-03  4.89822013e-01  0.00000000e+00\n",
      "   0.00000000e+00  1.18367544e-02  0.00000000e+00  0.00000000e+00\n",
      "  -3.46506942e-01 -4.81374555e-01]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(coefficients)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to display the coefficients with their corresponding feature names\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoefficient\u001b[39m\u001b[38;5;124m'\u001b[39m: coefficients})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importance)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    730\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    731\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    732\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:664\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    662\u001b[0m         raw_lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m val\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "PendingRollbackError",
     "evalue": "Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPendingRollbackError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(database_connection)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Insert the dataframe into the PostgreSQL table\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m'\u001b[39m, engine, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:3008\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3005\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[1;32m   3009\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3010\u001b[0m     name,\n\u001b[1;32m   3011\u001b[0m     con,\n\u001b[1;32m   3012\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m   3013\u001b[0m     if_exists\u001b[38;5;241m=\u001b[39mif_exists,\n\u001b[1;32m   3014\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   3015\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[1;32m   3016\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m   3017\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   3018\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   3019\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:787\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, DataFrame):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    785\u001b[0m     )\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mto_sql(\n\u001b[1;32m    789\u001b[0m         frame,\n\u001b[1;32m    790\u001b[0m         name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_kwargs,\n\u001b[1;32m    800\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:1585\u001b[0m, in \u001b[0;36mSQLDatabase.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturns_generator:\n\u001b[0;32m-> 1585\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit_stack\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:597\u001b[0m, in \u001b[0;36mExitStack.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Immediately unwind the context stack.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:589\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/contextlib.py:574\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb(\u001b[38;5;241m*\u001b[39mexc_details):\n\u001b[1;32m    575\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    576\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/util.py:146\u001b[0m, in \u001b[0;36mTransactionalContext.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollback_can_be_called():\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollback()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/util.py:144\u001b[0m, in \u001b[0;36mTransactionalContext.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction_is_active():\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2615\u001b[0m, in \u001b[0;36mTransaction.commit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Commit this :class:`.Transaction`.\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \n\u001b[1;32m   2601\u001b[0m \u001b[38;5;124;03mThe implementation of this may vary based on the type of transaction in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2612\u001b[0m \n\u001b[1;32m   2613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_commit()\n\u001b[1;32m   2616\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2617\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_active\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2720\u001b[0m, in \u001b[0;36mRootTransaction._do_commit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2720\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_commit_impl()\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;66;03m# whether or not commit succeeds, cancel any\u001b[39;00m\n\u001b[1;32m   2723\u001b[0m     \u001b[38;5;66;03m# nested transactions, make this transaction \"inactive\"\u001b[39;00m\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;66;03m# and remove it as a reset agent\u001b[39;00m\n\u001b[1;32m   2725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39m_nested_transaction:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2691\u001b[0m, in \u001b[0;36mRootTransaction._connection_commit_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_connection_commit_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2691\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection\u001b[38;5;241m.\u001b[39m_commit_impl()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1134\u001b[0m, in \u001b[0;36mConnection._commit_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_commit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection)\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(e, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2342\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2341\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1132\u001b[0m, in \u001b[0;36mConnection._commit_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMMIT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_commit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection)\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(e, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:573\u001b[0m, in \u001b[0;36mConnection.connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_revalidate_connection()\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (exc\u001b[38;5;241m.\u001b[39mPendingRollbackError, exc\u001b[38;5;241m.\u001b[39mResourceClosedError):\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:665\u001b[0m, in \u001b[0;36mConnection._revalidate_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__can_reconnect \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalidated:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 665\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid_transaction()\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mraw_connection()\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sqlalchemy/engine/base.py:655\u001b[0m, in \u001b[0;36mConnection._invalid_transaction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invalid_transaction\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mPendingRollbackError(\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reconnect until invalid \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124mtransaction is rolled \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mback.  Please rollback() fully before proceeding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msavepoint \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nested_transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    659\u001b[0m         code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8s2b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    660\u001b[0m     )\n",
      "\u001b[0;31mPendingRollbackError\u001b[0m: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Read the Parquet file\n",
    "df = pd.read_parquet('2023_siri_data/concatenated_data_parquet/output_1_from_2023-01-01.00_at_0_to_2023-01-03.05_at_69983.parquet')\n",
    "\n",
    "# Create the connection string\n",
    "database_username = 'postgres'\n",
    "database_password = 'postgres'\n",
    "database_ip       = 'localhost'\n",
    "database_name     = 'siri_2023'\n",
    "database_connection = f'postgresql://{database_username}:{database_password}@{database_ip}/{database_name}'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(database_connection)\n",
    "\n",
    "# Insert the dataframe into the PostgreSQL table\n",
    "df.to_sql('processed_data', engine, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
